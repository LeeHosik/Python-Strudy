{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "- Simple! HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "<p>웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://zeushahn.github.io/Test/python/bs_exam01.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "h1 = 스크레이핑이란?\n",
      "h1 = <h1>스크레이핑이란?</h1>\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분만 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "# 요소의 글자 출력하기 : string, text\n",
    "print('h1 = '+h1.string)\n",
    "print('h1 = '+h1.text)\n",
    "print(f'h1 = {h1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 = 웹 페이지를 분석하는 것\n",
      "p2 = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "print('p1 = '+ p1.string)\n",
    "print('p2 = '+ p2.string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "<p id=\"body\">웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://zeushahn.github.io/Test/python/bs_exam02.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "title = 스크레이핑이란?\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(id=\"title\")\n",
    "print(title)\n",
    "print(\"title = \" +title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body = 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "body = soup.find(id=\"body\")\n",
    "print(\"body = \"+body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p id=\"body\">웹 페이지를 분석하는 것</p>, <p>원하는 부분을 추출하는 것</p>]\n"
     ]
    }
   ],
   "source": [
    "p = soup.findAll(\"p\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>원하는 부분을 추출하는 것</p>\n"
     ]
    }
   ],
   "source": [
    "원하는부분 = soup.find(id=\"body\").next_sibling.next_sibling\n",
    "print(원하는부분)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<ul>\n",
      "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
      "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://zeushahn.github.io/Test/python/bs_exam03.html'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.findAll(\"a\")\n",
    "\n",
    "for a in links:\n",
    "    text = a.string\n",
    "    href = a.attrs['href']\n",
    "    print(text, \">\",href) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luchesia/miniforge3/envs/tensorflow/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"title\")\n",
    "print(title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "○ (하늘상태) 22일(일)과 26일(목)은 대체로 흐리겠고 그 밖의 날은 대체로 맑겠습니다.<br />○ (기온) 이번 예보기간 중, 24일(화)~25일(수) 아침 기온은 -18~-12도, 낮 기온은 -9~-3도로 평년(최저기온 -12~-5도, 최고기온 1~3도)보다 낮겠고, <br />          그 밖의 날 아침 기온은 -15~-4도, 낮 기온은 -2~3도로 평년과 비슷하거나 조금 낮겠습니다.<br />○ (해상) 서해중부해상의 물결은 23일(월)~24일(화)은 1.0~4.0m로 매우 높게 일겠고, 25일(수)은 1.0~3.0m로 높게 일겠습니다. 그 밖의 날은 1.0~2.0m로 일겠습니다.<br />○ (주말전망) 21일(토)은 맑겠고, 22일(일)은 흐리겠습니다. 아침 기온은 -15~-4도, 낮 기온은 -2~3도가 되겠습니다.<br /><br />* 이번 예보기간 북쪽에서 남하하는 기압골과 찬 대륙고기압의 발달 정도 및 이동 속도에 따라 예보가 변경될 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(\"wf\")\n",
    "print(title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (하늘상태) 22일(일)과 26일(목)은 대체로 흐리겠고 그 밖의 날은 대체로 맑겠습니다.\n",
      " (기온) 이번 예보기간 중, 24일(화)~25일(수) 아침 기온은 -18~-12도, 낮 기온은 -9~-3도로 평년(최저기온 -12~-5도, 최고기온 1~3도)보다 낮겠고,           그 밖의 날 아침 기온은 -15~-4도, 낮 기온은 -2~3도로 평년과 비슷하거나 조금 낮겠습니다.\n",
      " (해상) 서해중부해상의 물결은 23일(월)~24일(화)은 1.0~4.0m로 매우 높게 일겠고, 25일(수)은 1.0~3.0m로 높게 일겠습니다. 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      " (주말전망) 21일(토)은 맑겠고, 22일(일)은 흐리겠습니다. 아침 기온은 -15~-4도, 낮 기온은 -2~3도가 되겠습니다.* 이번 예보기간 북쪽에서 남하하는 기압골과 찬 대륙고기압의 발달 정도 및 이동 속도에 따라 예보가 변경될 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# sents = []\n",
    "# for sen in title.text.split(sep=\"○\"):\n",
    "#     sen = sen.replace('<br />',\"\")\n",
    "#     print(sen)\n",
    "#     sents.append(sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (하늘상태) 22일(일)과 26일(목)은 대체로 흐리겠고 그 밖의 날은 대체로 맑겠습니다.\n",
      " (기온) 이번 예보기간 중, 24일(화)~25일(수) 아침 기온은 -18~-12도, 낮 기온은 -9~-3도로 평년(최저기온 -12~-5도, 최고기온 1~3도)보다 낮겠고, \n",
      "          그 밖의 날 아침 기온은 -15~-4도, 낮 기온은 -2~3도로 평년과 비슷하거나 조금 낮겠습니다.\n",
      " (해상) 서해중부해상의 물결은 23일(월)~24일(화)은 1.0~4.0m로 매우 높게 일겠고, 25일(수)은 1.0~3.0m로 높게 일겠습니다. 그 밖의 날은 1.0~2.0m로 일겠습니다.\n",
      " (주말전망) 21일(토)은 맑겠고, 22일(일)은 흐리겠습니다. 아침 기온은 -15~-4도, 낮 기온은 -2~3도가 되겠습니다.\n",
      "\n",
      "* 이번 예보기간 북쪽에서 남하하는 기압골과 찬 대륙고기압의 발달 정도 및 이동 속도에 따라 예보가 변경될 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "wf = soup.find('wf').text.replace('<br />','\\n').replace(\"○\",\"\")\n",
    "print(wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d864e386a55a2cee39c31bc0e2325312cb68f97ec75faaaf5382620c119f58c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
